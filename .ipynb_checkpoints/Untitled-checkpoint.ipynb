{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Ciencia dos Dados - Star Wars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Nome: Daniel Gurgel Terra\n",
    ">   \n",
    ">Nome: Fernando Giuseppe Avila Beltramo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classificador automático de sentimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente no Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAÇÕES\n",
    "\n",
    "import tweepy #twitter\n",
    "import math #op matematicas\n",
    "import os #op arquivos\n",
    "import pandas as pd #DataFrame, Series\n",
    "import json #manipulação de arquivos\n",
    "import re #expressões regulares\n",
    "import string #str\n",
    "import datetime\n",
    "from random import shuffle #random import embaralhar\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticação no Twitter\n",
    "\n",
    "* Daniel Gurgel Terra : ***@LandWhale_Watch***\n",
    "* Fernando Giuseppe Avila Beltramo : ***@AttackHeli_ItMe***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza a API de uma conta qqr\n",
    "def Make_API(file_name):\n",
    "    \n",
    "    # JSON com as info das contas\n",
    "    with open(file_name) as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    # Biblioteca de Auth <Não modificar>\n",
    "    auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "    auth.set_access_token( data['access_token'], data['access_token_secret'])\n",
    "    \n",
    "    # retorna a API\n",
    "    return tweepy.API(auth)\n",
    "\n",
    "# Conta @AttackHeli_ItMe\n",
    "API_AHIM = Make_API('AHIM_auth.pass')\n",
    "\n",
    "# Conta @LandWhale_Watch\n",
    "API_LWW = Make_API('LWW_auth.pass')\n",
    "\n",
    "API_list = [API_AHIM, API_LWW]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Etapas do projeto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Função *Coleta_Tweet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variaveis de suporte para a captura de Tweets\n",
    "ultimo_id =-1\n",
    "ultima_data = datetime.datetime.now()\n",
    "\n",
    "# Tempo para atualização do ultimo_id (em minutos)\n",
    "tempo_de_reset_do_ultimo_id = 60\n",
    "\n",
    "# Função para a captura de Tweets\n",
    "def getTweets(palavra_de_procura, quantidade_de_procuras_por_api, ultimo_id, ultima_data):\n",
    "    # Lista do resultado final da captura\n",
    "    resultado = []\n",
    "    \n",
    "    # Numero da API que ta sendo usada\n",
    "    numero_api = 0\n",
    "    \n",
    "    # Captura o numero de tweets adicionados por API\n",
    "    resultado_api = {}\n",
    "    \n",
    "    if ultima_data <= datetime.datetime.now():\n",
    "        ultima_data = datetime.datetime.now() + datetime.timedelta(minutes=tempo_de_reset_do_ultimo_id)\n",
    "        ultimo_id = -1\n",
    "        \n",
    "        print(\"AVISO: Houve um reinicio do ultimo_id, data_atual =\", ultima_data)\n",
    "    \n",
    "    # Para cada API gerada\n",
    "    for api in API_list:\n",
    "        numero_api = numero_api + 1\n",
    "        \n",
    "        # Contagem de captura para a API\n",
    "        contagem = 0\n",
    "        \n",
    "        # Inicia a contagem para a API\n",
    "        resultado_api[numero_api] = 0\n",
    "        \n",
    "        if ultimo_id < 0:\n",
    "            cursor = tweepy.Cursor(api.search, q=palavra_de_procura, lang=\"en\", tweet_mode=\"extended\")\n",
    "        else:\n",
    "            cursor = tweepy.Cursor(api.search, q=palavra_de_procura, lang=\"en\", tweet_mode=\"extended\", max_id=ultimo_id)\n",
    "        \n",
    "        # Captura de um novo tweet\n",
    "        for tweet in cursor.items(quantidade_de_procuras_por_api):\n",
    "            \n",
    "            # Texto do tweet capturado\n",
    "            texto = tweet.full_text.lower()\n",
    "            \n",
    "            texto = re.sub( \"http(|s):\\/\\/[^ \\n]*\", \"\", texto)\n",
    "                \n",
    "            # Verfica se a palavra de procura se encontra no texto:\n",
    "            if palavra_de_procura.lower() in texto:\n",
    "                    \n",
    "                # Não adiciona tweet duplicado\n",
    "                if texto not in resultado:\n",
    "                        \n",
    "                    # Adiciona o texto ao resultado\n",
    "                    resultado.append(texto)\n",
    "                        \n",
    "                    # Adiciona as capturas da api + 1\n",
    "                    resultado_api[numero_api] = resultado_api[numero_api] + 1\n",
    "                \n",
    "            # Adiciona 1 captura a contagem de capturas para esta API\n",
    "            contagem = contagem + 1\n",
    "                \n",
    "            print(\"API {0} de {1} - {2:.0f}% das capturas realizadas, {3} tweets validos\".format(\n",
    "                  numero_api, len(API_list), (contagem/quantidade_de_procuras_por_api)*100, len(resultado)), end=\"\\r\")\n",
    "            \n",
    "            # Captura o ultimo id da captura para não duplicar pedido\n",
    "            if contagem == quantidade_de_procuras_por_api:\n",
    "                ultimo_id = tweet.id\n",
    "            \n",
    "    # Embaralhando as mensagens para reduzir um possível viés\n",
    "    shuffle(resultado)\n",
    "    \n",
    "    for api, quantidade in resultado_api.items():\n",
    "        print(\"A API {0} capturou {1} tweets de {2}, equivalente a {3:.2f}% do total\".format(api, quantidade, len(resultado), quantidade/len(resultado)*100))\n",
    "    \n",
    "    return resultado, ultimo_id, ultima_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Função *Limpa_Tweet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpando os termos insignificantes de cada tweet, substitui por \"\"\n",
    "def Limpa_Tweet(lista_tweets, procura):\n",
    "    \n",
    "    # Remove Menções\n",
    "    lista_tweets = re.sub( \"@[^ ]*\", \"\", lista_tweets)\n",
    "    \n",
    "    # Remove Links\n",
    "    lista_tweets = re.sub( \"http(|s):\\/\\/[^ \\n]*\", \"\", lista_tweets)\n",
    "    \n",
    "    # Remove Pontuações\n",
    "    lista_tweets = re.sub( \"[\" + string.punctuation + \"]\", \"\", lista_tweets)\n",
    "    \n",
    "    # Remove Procura\n",
    "    lista_tweets = re.sub( procura.lower(), \"\", lista_tweets)\n",
    "    \n",
    "    # Lista Limpa\n",
    "    return lista_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Escolha de uma Marca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Marca e Quantidade de coleta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marca\n",
    "procura = \"Star Wars\"\n",
    "\n",
    "# Quantidade de Tweets\n",
    "quantidade = 500\n",
    "\n",
    "# Linguagem\n",
    "linguagem = \"en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Criando nome para o Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel gerados\n",
    "captura = 1\n",
    "\n",
    "# Verifica se no diretório há outro excel\n",
    "for file in os.listdir(\".\"):\n",
    "    if procura in file:\n",
    "        captura +=1\n",
    "\n",
    "# Nomeia o arquivo\n",
    "nome_arquivo = procura + \"_TREINO_\" + str(captura) + \".xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Coletando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista_tweets = Coleta_Tweet(procura,quantidade,linguagem)\n",
    "\n",
    "#tabela = pd.DataFrame({\"tweets\": pd.Series(lista_tweets), \"valor\": [\"\"]*len(lista_tweets)})\n",
    "\n",
    "#tabela.to_excel(nome_arquivo, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando um Classificador Naive-Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Considerando apenas as mensagens da planilha de Treinamento, ensine seu classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma os excels avaliados em um Classificador\n",
    "\n",
    "def criar_naive_bayes(excels, marca):\n",
    "\n",
    "    tabela_processada = pd.DataFrame({\"tweets\": [\"\"], \"valor\": [\"\"], \"valor atribuido automaticamente\": [\"\"]})\n",
    "    \n",
    "    # Junta todos os treinamentos em um\n",
    "    for tabela in excels:\n",
    "        tabela_processada = pd.concat([tabela_processada, pd.read_excel(tabela)], sort=False)\n",
    "    \n",
    "    tabela_processada = tabela_processada.iloc[1:]\n",
    "    \n",
    "    tamanho_original = len(tabela_processada)\n",
    "    \n",
    "    # Remove todos Tweets duplicados\n",
    "    tabela_processada = tabela_processada.drop_duplicates(subset=\"tweets\")\n",
    "    \n",
    "    print(\"Foram removidas {0} linhas duplicadas para a formação do Naive Bayes\".format(tamanho_original-len(tabela_processada)))\n",
    "    \n",
    "    palavras = Limpa_Tweet(tabela_processada[\"tweets\"].str.cat(), marca).split()\n",
    "    \n",
    "    # Variavel de apoio do Laplace\n",
    "    todas_as_palavras = set(palavras)\n",
    "    \n",
    "    categorias = set(tabela_processada[\"valor\"])\n",
    "    \n",
    "    dict_resultado = {}\n",
    "    \n",
    "    for categoria in categorias:\n",
    "\n",
    "        sub_tabela = tabela_processada[tabela_processada[\"valor\"] == categoria]\n",
    "        \n",
    "        mensagens = Limpa_Tweet(sub_tabela[\"tweets\"].str.cat(), marca)\n",
    "\n",
    "        series = pd.Series(mensagens.split())\n",
    "        \n",
    "        prior = len(sub_tabela.index)/len(tabela_processada.index)\n",
    "        \n",
    "        print(\"O Prior da categoria {0} é de {1:.2f}%\".format(categoria, prior*100))\n",
    "        \n",
    "        dict_resultado[categoria] = [series.value_counts(), prior]\n",
    "    \n",
    "    return dict_resultado, todas_as_palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teste(novos_tweets, dict_de_atribuicoes, todas_as_palavras, marca):\n",
    "    resultado = []\n",
    "\n",
    "    for tweet in novos_tweets:\n",
    "\n",
    "        probabilidades = {}\n",
    "\n",
    "        for palavra in Limpa_Tweet(tweet, marca).split():\n",
    "\n",
    "            for categoria, dados_categoria in dict_de_atribuicoes.items():\n",
    "\n",
    "                series_da_categoria = dados_categoria[0]\n",
    "\n",
    "                if palavra in series_da_categoria:\n",
    "                    probabilidade = (series_da_categoria[palavra] + 1) / (len(series_da_categoria) +  len(todas_as_palavras))\n",
    "                else:\n",
    "                    probabilidade = 1 / (len(series_da_categoria) + len(todas_as_palavras))\n",
    "\n",
    "                if categoria in probabilidades:\n",
    "                    probabilidades[categoria] = probabilidades[categoria] * probabilidade\n",
    "                else:\n",
    "\n",
    "                    probabilidades[categoria] = probabilidade * dados_categoria[1]\n",
    "\n",
    "        if len(probabilidades) == 0:\n",
    "            resultado.append(-1)\n",
    "        else:\n",
    "            resultado.append(max(probabilidades, key=probabilidades.get))\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Ultilizando as funções\n",
    "\n",
    "Testando todas as funções criadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Numero_de_tentativas = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(tabelas_avaliadas, ultimo_id, ultima_data, Numero_de_tentativas):\n",
    "    # Adiciona esta tentativa\n",
    "    Numero_de_tentativas = Numero_de_tentativas + 1\n",
    "    \n",
    "    # Captura novos tweets\n",
    "    novos_tweets, ultimo_id, ultima_data = getTweets(procura, quantidade, ultimo_id, ultima_data)\n",
    "\n",
    "    # Cria o classifcador\n",
    "    classificador, todas_as_palavras = criar_naive_bayes(tabelas_avaliadas, procura)\n",
    "\n",
    "    # Cria a auto_avaliação\n",
    "    auto_avaliação_dos_novos_tweets = teste(novos_tweets, classificador, todas_as_palavras, procura)\n",
    "\n",
    "    # Cria o nome do excel a ser criado\n",
    "    nome_do_arquivo_de_teste = procura + \"_TESTE_\" + str(Numero_de_tentativas) + \".xlsx\"\n",
    "\n",
    "    # Cria a tabela que vai ser salva no excel\n",
    "    tabela = pd.DataFrame({\"tweets\": pd.Series(novos_tweets), \"valor\": [\"\"]*len(novos_tweets),\n",
    "                           \"valor atribuido automaticamente\": auto_avaliação_dos_novos_tweets})\n",
    "\n",
    "    # Savalva a tabela no excel\n",
    "    tabela.to_excel(nome_do_arquivo_de_teste, index=False)\n",
    "\n",
    "    print(\"O Excel\", nome_do_arquivo_de_teste, \"foi gerado!\")\n",
    "    \n",
    "    return ultimo_id, ultima_data, Numero_de_tentativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVISO: Houve um reinicio do ultimo_id, data_atual = 2019-09-17 20:55:23.548948\n",
      "A API 1 capturou 217 tweets de 400, equivalente a 54.25% do total\n",
      "A API 2 capturou 183 tweets de 400, equivalente a 45.75% do total\n",
      "Foram removidas 0 linhas duplicadas para a formação do Naive Bayes\n",
      "O Prior da categoria VIDEO é de 1.75%\n",
      "O Prior da categoria REL é de 8.76%\n",
      "O Prior da categoria G+ é de 6.21%\n",
      "O Prior da categoria D+ é de 2.07%\n",
      "O Prior da categoria SW- é de 2.71%\n",
      "O Prior da categoria POD é de 0.32%\n",
      "O Prior da categoria MERCH é de 12.42%\n",
      "O Prior da categoria HS+ é de 2.71%\n",
      "O Prior da categoria SQ- é de 6.53%\n",
      "O Prior da categoria D- é de 3.82%\n",
      "O Prior da categoria SQ+ é de 5.25%\n",
      "O Prior da categoria TV+ é de 1.43%\n",
      "O Prior da categoria OT+ é de 1.27%\n",
      "O Prior da categoria OT- é de 0.32%\n",
      "O Prior da categoria EU- é de 0.48%\n",
      "O Prior da categoria RAND é de 25.64%\n",
      "O Prior da categoria ART é de 2.55%\n",
      "O Prior da categoria SW+ é de 10.83%\n",
      "O Prior da categoria PQ- é de 0.80%\n",
      "O Prior da categoria EU+ é de 1.27%\n",
      "O Prior da categoria HS- é de 0.32%\n",
      "O Prior da categoria G- é de 0.32%\n",
      "O Prior da categoria PQ+ é de 2.23%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ZIP does not support timestamps before 1980",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-626086aabe0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtabelas_avaliadas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Star Wars_TREINO_1.xlsx\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0multimo_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0multima_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNumero_de_tentativas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtabelas_avaliadas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0multimo_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0multima_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNumero_de_tentativas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-60-e2249f5b92b9>\u001b[0m in \u001b[0;36mrun_all\u001b[1;34m(tabelas_avaliadas, ultimo_id, ultima_data, Numero_de_tentativas)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Savalva a tabela no excel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mtabela\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnome_do_arquivo_de_teste\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"O Excel\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnome_do_arquivo_de_teste\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"foi gerado!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes)\u001b[0m\n\u001b[0;32m   2125\u001b[0m         formatter.write(excel_writer, sheet_name=sheet_name, startrow=startrow,\n\u001b[0;32m   2126\u001b[0m                         \u001b[0mstartcol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstartcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreeze_panes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfreeze_panes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2127\u001b[1;33m                         engine=engine)\n\u001b[0m\u001b[0;32m   2128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2129\u001b[0m     def to_json(self, path_or_buf=None, orient=None, date_format=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\excel.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine)\u001b[0m\n\u001b[0;32m    662\u001b[0m                            freeze_panes=freeze_panes)\n\u001b[0;32m    663\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mneed_save\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m             \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1950\u001b[0m         \"\"\"\n\u001b[0;32m   1951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1952\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1954\u001b[0m     def write_cells(self, cells, sheet_name=None, startrow=0, startcol=0,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileclosed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileclosed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_store_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py\u001b[0m in \u001b[0;36m_store_workbook\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    677\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 679\u001b[1;33m                 \u001b[0mxlsx_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxml_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    680\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[0;32m   1708\u001b[0m             )\n\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1710\u001b[1;33m         \u001b[0mzinfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mZipInfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marcname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1712\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mzinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36mfrom_file\u001b[1;34m(cls, filename, arcname)\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misdir\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[0marcname\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m         \u001b[0mzinfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marcname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m         \u001b[0mzinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternal_attr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mst_mode\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;36m0xFFFF\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<<\u001b[0m \u001b[1;36m16\u001b[0m  \u001b[1;31m# Unix attributes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misdir\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, date_time)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdate_time\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1980\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ZIP does not support timestamps before 1980'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;31m# Standard values:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: ZIP does not support timestamps before 1980"
     ]
    }
   ],
   "source": [
    "tabelas_avaliadas = []\n",
    "\n",
    "tabelas_avaliadas.append(\"Star Wars_TREINO_1.xlsx\")\n",
    "\n",
    "ultimo_id, ultima_data, Numero_de_tentativas = run_all(tabelas_avaliadas, ultimo_id, ultima_data, Numero_de_tentativas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
