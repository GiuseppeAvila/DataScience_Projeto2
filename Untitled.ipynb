{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Ciencia dos Dados - Star Wars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Nome: Daniel Gurgel Terra\n",
    ">   \n",
    ">Nome: Fernando Giuseppe Avila Beltramo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classificador automático de sentimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente no Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAÇÕES\n",
    "\n",
    "import tweepy #twitter\n",
    "import math #op matematicas\n",
    "import os #op arquivos\n",
    "import pandas as pd #DataFrame, Series\n",
    "import json #manipulação de arquivos\n",
    "import re #expressões regulares\n",
    "import string #str\n",
    "from random import shuffle #random import embaralhar\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticação no Twitter\n",
    "\n",
    "* Daniel Gurgel Terra : ***@LandWhale_Watch***\n",
    "* Fernando Giuseppe Avila Beltramo : ***@AttackHeli_ItMe***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza a API de uma conta qqr\n",
    "def Make_API(file_name):\n",
    "    \n",
    "    # JSON com as info das contas\n",
    "    with open(file_name) as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    # Biblioteca de Auth <Não modificar>\n",
    "    auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "    auth.set_access_token( data['access_token'], data['access_token_secret'])\n",
    "    \n",
    "    # retorna a API\n",
    "    return tweepy.API(auth)\n",
    "\n",
    "# Conta @AttackHeli_ItMe\n",
    "API_AHIM = Make_API('AHIM_auth.pass')\n",
    "\n",
    "# Conta @LandWhale_Watch\n",
    "API_LWW = Make_API('LWW_auth.pass')\n",
    "\n",
    "API_list = [API_AHIM, API_LWW]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Etapas do projeto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Função *Coleta_Tweet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_tweet_id = -1\n",
    "\n",
    "# Coleta dos Tweets\n",
    "def Coleta_Tweet(procura, quantidade, linguagem):\n",
    "    \n",
    "    # Resultado\n",
    "    resultado = []\n",
    "    \n",
    "    api_value = 0\n",
    "    \n",
    "    api_result = {}\n",
    "    \n",
    "    # Para cada API das Contas:\n",
    "    for api in API_list:\n",
    "        \n",
    "        api_value += 1\n",
    "        \n",
    "        # Quantidade de tweets coletados nesta conta\n",
    "        contador = 0\n",
    "        \n",
    "        api_result[api_value] = 0\n",
    "        \n",
    "        if last_tweet_id < 0:\n",
    "            cursor = tweepy.Cursor(api.search, q=procura, lang=linguagem, tweet_mode=\"extended\")\n",
    "        else:\n",
    "            cursor = tweepy.Cursor(api.search, q=procura, lang=linguagem, tweet_mode=\"extended\", max_id = last_tweet_id)\n",
    "        \n",
    "        # Captura de um tweet que contenha a 'procura'\n",
    "        for tweet in cursor.items(quantidade):\n",
    "            \n",
    "            frase = tweet.full_text.lower()\n",
    "            \n",
    "            frase = re.sub( \"http(|s):\\/\\/[^ \\n]*\", \"\", frase)\n",
    "               \n",
    "            # Se procura está em texto, bloqueia a repetição \n",
    "            if procura.lower() in frase:\n",
    "                if frase not in resultado:\n",
    "                    resultado.append(frase)\n",
    "                    api_result[api_value] += 1\n",
    "                    \n",
    "            # Conta o tweet:\n",
    "            contador +=1\n",
    "            \n",
    "            # Barra de loading\n",
    "            Realizado = (contador/quantidade)*100\n",
    "            print(\"API {0}, {1} tweets: {2:.0f}% REALIZADO\".format(api_value, len(resultado), Realizado), end='\\r')\n",
    "            \n",
    "            if contador == quantidade:\n",
    "                last_tweet_id = tweet.id\n",
    "            \n",
    "    # Embaralha o resultado (impedir tendência)\n",
    "    shuffle(resultado)\n",
    "    \n",
    "    for api, total in api_result.items():\n",
    "        print(\"API {0}, com {1} tweets de {2} ({3}% do total)\".format(api, total, len(resultado), total/len(resultado)*100))\n",
    "    \n",
    "    # Retorna os tweets\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variaveis de suporte para a captura de Tweets\n",
    "ultimo_id =-1\n",
    "ultima_data = datetime.datetime.now()\n",
    "\n",
    "# Tempo para atualização do ultimo_id (em minutos)\n",
    "tempo_de_reset_do_ultimo_id = 60\n",
    "\n",
    "# Função para a captura de Tweets\n",
    "def getTweets(palavra_de_procura, quantidade_de_procuras_por_api, ultimo_id, ultima_data):\n",
    "    # Lista do resultado final da captura\n",
    "    resultado = []\n",
    "    \n",
    "    # Numero da API que ta sendo usada\n",
    "    numero_api = 0\n",
    "    \n",
    "    # Captura o numero de tweets adicionados por API\n",
    "    resultado_api = {}\n",
    "    \n",
    "    if ultima_data <= datetime.datetime.now():\n",
    "        ultima_data = datetime.datetime.now() + datetime.timedelta(minutes=tempo_de_reset_do_ultimo_id)\n",
    "        ultimo_id = -1\n",
    "        \n",
    "        print(\"AVISO: Houve um reinicio do ultimo_id, data_atual =\", ultima_data)\n",
    "    \n",
    "    # Para cada API gerada\n",
    "    for api in apis:\n",
    "        numero_api = numero_api + 1\n",
    "        \n",
    "        # Contagem de captura para a API\n",
    "        contagem = 0\n",
    "        \n",
    "        # Inicia a contagem para a API\n",
    "        resultado_api[numero_api] = 0\n",
    "        \n",
    "        if ultimo_id < 0:\n",
    "            cursor = tweepy.Cursor(api.search, q=palavra_de_procura, lang=\"pt\", tweet_mode=\"extended\")\n",
    "        else:\n",
    "            cursor = tweepy.Cursor(api.search, q=palavra_de_procura, lang=\"pt\", tweet_mode=\"extended\", max_id=ultimo_id)\n",
    "        \n",
    "        # Captura de um novo tweet\n",
    "        for tweet in cursor.items(quantidade_de_procuras_por_api):\n",
    "            \n",
    "            # Texto do tweet capturado\n",
    "            texto = tweet.full_text.lower()\n",
    "            \n",
    "            texto = re.sub( \"http(|s):\\/\\/[^ \\n]*\", \"\", texto)\n",
    "                \n",
    "            # Verfica se a palavra de procura se encontra no texto:\n",
    "            if palavra_de_procura.lower() in texto:\n",
    "                    \n",
    "                # Não adiciona tweet duplicado\n",
    "                if texto not in resultado:\n",
    "                        \n",
    "                    # Adiciona o texto ao resultado\n",
    "                    resultado.append(texto)\n",
    "                        \n",
    "                    # Adiciona as capturas da api + 1\n",
    "                    resultado_api[numero_api] = resultado_api[numero_api] + 1\n",
    "                \n",
    "            # Adiciona 1 captura a contagem de capturas para esta API\n",
    "            contagem = contagem + 1\n",
    "                \n",
    "            print(\"API {0} de {1} - {2:.0f}% das capturas realizadas, {3} tweets validos\".format(\n",
    "                  numero_api, len(apis), (contagem/quantidade_de_procuras_por_api)*100, len(resultado)), end=\"\\r\")\n",
    "            \n",
    "            # Captura o ultimo id da captura para não duplicar pedido\n",
    "            if contagem == quantidade_de_procuras_por_api:\n",
    "                ultimo_id = tweet.id\n",
    "            \n",
    "    # Embaralhando as mensagens para reduzir um possível viés\n",
    "    shuffle(resultado)\n",
    "    \n",
    "    for api, quantidade in resultado_api.items():\n",
    "        print(\"A API {0} capturou {1} tweets de {2}, equivalente a {3:.2f}% do total\".format(api, quantidade, len(resultado), quantidade/len(resultado)*100))\n",
    "    \n",
    "    return resultado, ultimo_id, ultima_data\n",
    "\n",
    "# Função para limpar um texto de um Tweet\n",
    "def limpar_texto(texto, marca):\n",
    "    # Remove Menções\n",
    "    texto = re.sub(\"@[^ ]*\", \"\", texto)\n",
    "    \n",
    "    # Remove links\n",
    "    texto = re.sub(\"http(|s):\\/\\/[^ \\n]*\", \"\", texto)\n",
    "    \n",
    "    # Remove Pontuações\n",
    "    texto = re.sub(\"[\" + string.punctuation + \"]+\", \"\", texto)\n",
    "    \n",
    "    # Remove a marca analisada\n",
    "    return re.sub(marca.lower(), \"\", texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Função *Limpa_Tweet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpando os termos insignificantes de cada tweet, substitui por \"\"\n",
    "def Limpa_Tweet(lista_tweets, procura):\n",
    "    \n",
    "    # Remove Menções\n",
    "    lista_tweets = re.sub( \"@[^ ]*\", \"\", lista_tweets)\n",
    "    \n",
    "    # Remove Links\n",
    "    lista_tweets = re.sub( \"http(|s):\\/\\/[^ \\n]*\", \"\", lista_tweets)\n",
    "    \n",
    "    # Remove Pontuações\n",
    "    lista_tweets = re.sub( \"[\" + string.punctuation + \"]\", \"\", lista_tweets)\n",
    "    \n",
    "    # Remove Procura\n",
    "    lista_tweets = re.sub( procura.lower(), \"\", lista_tweets)\n",
    "    \n",
    "    # Lista Limpa\n",
    "    return lista_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Escolha de uma Marca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Marca e Quantidade de coleta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marca\n",
    "procura = \"Star Wars\"\n",
    "\n",
    "# Quantidade de Tweets\n",
    "quantidade = 500\n",
    "\n",
    "# Linguagem\n",
    "linguagem = \"en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Criando nome para o Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel gerados\n",
    "captura = 1\n",
    "\n",
    "# Verifica se no diretório há outro excel\n",
    "for file in os.listdir(\".\"):\n",
    "    if procura in file:\n",
    "        captura +=1\n",
    "\n",
    "# Nomeia o arquivo\n",
    "nome_arquivo = procura + \"_TREINO_\" + str(captura) + \".xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Coletando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'last_tweet_id' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-1feac1673352>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlista_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mColeta_Tweet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocura\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquantidade\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlinguagem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtabela\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"tweets\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlista_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"valor\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlista_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtabela\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnome_arquivo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-3dd6416393cc>\u001b[0m in \u001b[0;36mColeta_Tweet\u001b[1;34m(procura, quantidade, linguagem)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mapi_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mapi_value\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlast_tweet_id\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprocura\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlinguagem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweet_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"extended\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'last_tweet_id' referenced before assignment"
     ]
    }
   ],
   "source": [
    "lista_tweets = Coleta_Tweet(procura,quantidade,linguagem)\n",
    "\n",
    "tabela = pd.DataFrame({\"tweets\": pd.Series(lista_tweets), \"valor\": [\"\"]*len(lista_tweets)})\n",
    "\n",
    "tabela.to_excel(nome_arquivo, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando um Classificador Naive-Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Considerando apenas as mensagens da planilha de Treinamento, ensine seu classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma os excels avaliados em um Classificador\n",
    "\n",
    "def criar_naive_bayes(excels, marca):\n",
    "\n",
    "    tabela_processada = pd.DataFrame({\"tweets\": [\"\"], \"valor\": [\"\"], \"valor atribuido automaticamente\": [\"\"]})\n",
    "    \n",
    "    # Junta todos os treinamentos em um\n",
    "    for tabela in excels:\n",
    "        tabela_processada = pd.concat([tabela_processada, pd.read_excel(tabela)], sort=False)\n",
    "    \n",
    "    tabela_processada = tabela_processada.iloc[1:]\n",
    "    \n",
    "    tamanho_original = len(tabela_processada)\n",
    "    \n",
    "    # Remove todos Tweets duplicados\n",
    "    tabela_processada = tabela_processada.drop_duplicates(subset=\"Tweets\")\n",
    "    \n",
    "    print(\"Foram removidas {0} linhas duplicadas para a formação do Naive Bayes\".format(tamanho_original-len(tabela_processada)))\n",
    "    \n",
    "    palavras = Limpa_Tweet(tabela_processada[\"Tweets\"].str.cat(), marca).split()\n",
    "    \n",
    "    # Variavel de apoio do Laplace\n",
    "    todas_as_palavras = set(palavras)\n",
    "    \n",
    "    categorias = set(tabela_processada[\"Categoria\"])\n",
    "    \n",
    "    dict_resultado = {}\n",
    "    \n",
    "    for categoria in categorias:\n",
    "\n",
    "        sub_tabela = tabela_processada[tabela_processada[\"Categoria\"] == categoria]\n",
    "        \n",
    "        mensagens = limpar_texto(sub_tabela[\"Tweets\"].str.cat(), marca)\n",
    "\n",
    "        series = pd.Series(mensagens.split())\n",
    "        \n",
    "        prior = len(sub_tabela.index)/len(tabela_processada.index)\n",
    "        \n",
    "        print(\"O Prior da categoria {0} é de {1:.2f}%\".format(categoria, prior*100))\n",
    "        \n",
    "        dict_resultado[categoria] = [series.value_counts(), prior]\n",
    "    \n",
    "    return dict_resultado, todas_as_palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teste(novos_tweets, dict_de_atribuicoes, todas_as_palavras, marca):\n",
    "    resultado = []\n",
    "\n",
    "    for tweet in novos_tweets:\n",
    "\n",
    "        probabilidades = {}\n",
    "\n",
    "        for palavra in Limpa_Tweet(tweet, marca).split():\n",
    "\n",
    "            for categoria, dados_categoria in dict_de_atribuicoes.items():\n",
    "\n",
    "                series_da_categoria = dados_categoria[0]\n",
    "\n",
    "                if palavra in series_da_categoria:\n",
    "                    probabilidade = (series_da_categoria[palavra] + 1) / (len(series_da_categoria) +  len(todas_as_palavras))\n",
    "                else:\n",
    "                    probabilidade = 1 / (len(series_da_categoria) + len(todas_as_palavras))\n",
    "\n",
    "                if categoria in probabilidades:\n",
    "                    probabilidades[categoria] = probabilidades[categoria] * probabilidade\n",
    "                else:\n",
    "\n",
    "                    probabilidades[categoria] = probabilidade * dados_categoria[1]\n",
    "\n",
    "        if len(probabilidades) == 0:\n",
    "            resultado.append(-1)\n",
    "        else:\n",
    "            resultado.append(max(probabilidades, key=probabilidades.get))\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Ultilizando as funções\n",
    "\n",
    "Testando todas as funções criadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Numero_de_tentativas = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(tabelas_avaliadas, ultimo_id, ultima_data, Numero_de_tentativas):\n",
    "    # Adiciona esta tentativa\n",
    "    Numero_de_tentativas = Numero_de_tentativas + 1\n",
    "    \n",
    "    # Captura novos tweets\n",
    "    novos_tweets, ultimo_id, ultima_data = getTweets(marca, quantidade_por_api, ultimo_id, ultima_data)\n",
    "\n",
    "    # Cria o classifcador\n",
    "    classificador, todas_as_palavras = criar_naive_bayes(tabelas_avaliadas, marca)\n",
    "\n",
    "    # Cria a auto_avaliação\n",
    "    auto_avaliação_dos_novos_tweets = executar_teste(novos_tweets, classificador, todas_as_palavras, marca)\n",
    "\n",
    "    # Cria o nome do excel a ser criado\n",
    "    nome_do_arquivo_de_teste = marca + \"-Teste-\" + str(Numero_de_tentativas) + \".xlsx\"\n",
    "\n",
    "    # Cria a tabela que vai ser salva no excel\n",
    "    tabela = pd.DataFrame({\"Tweets\": pd.Series(novos_tweets), \"Categoria\": [\"\"]*len(novos_tweets),\n",
    "                           \"Categoria atribuida automaticamente\": auto_avaliação_dos_novos_tweets})\n",
    "\n",
    "    # Savalva a tabela no excel\n",
    "    tabela.to_excel(nome_do_arquivo_de_teste, index=False)\n",
    "\n",
    "    print(\"O Excel\", nome_do_arquivo_de_teste, \"foi gerado!\")\n",
    "    \n",
    "    return ultimo_id, ultima_data, Numero_de_tentativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'last_tweet_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-f5c8d5035cf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtabelas_avaliadas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Star Wars_TREINO_1.xlsx\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0multimo_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0multima_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNumero_de_tentativas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtabelas_avaliadas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_tweet_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0multima_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNumero_de_tentativas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'last_tweet_id' is not defined"
     ]
    }
   ],
   "source": [
    "tabelas_avaliadas = []\n",
    "\n",
    "tabelas_avaliadas.append(\"Star Wars_TREINO_1.xlsx\")\n",
    "\n",
    "ultimo_id, ultima_data, Numero_de_tentativas = run_all(tabelas_avaliadas, ultimo_id, ultima_data, Numero_de_tentativas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
